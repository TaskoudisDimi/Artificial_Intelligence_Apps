Training iter #1500:   Batch Loss = 5.821078, Accuracy = 0.18133333325386047
PERFORMANCE ON TEST SET: Batch Loss = 5.240767955780029, Accuracy = 0.16423481702804565
Training iter #30000:   Batch Loss = 2.679716, Accuracy = 0.17733334004878998
PERFORMANCE ON TEST SET: Batch Loss = 2.6495561599731445, Accuracy = 0.17543263733386993
Training iter #60000:   Batch Loss = 2.534071, Accuracy = 0.15199999511241913
PERFORMANCE ON TEST SET: Batch Loss = 2.4952552318573, Accuracy = 0.1876484602689743
Training iter #90000:   Batch Loss = 2.463270, Accuracy = 0.15666666626930237
PERFORMANCE ON TEST SET: Batch Loss = 2.4589123725891113, Accuracy = 0.18425516784191132
Training iter #120000:   Batch Loss = 2.449020, Accuracy = 0.17866666615009308
PERFORMANCE ON TEST SET: Batch Loss = 2.437601089477539, Accuracy = 0.1947743445634842
Training iter #150000:   Batch Loss = 2.420522, Accuracy = 0.18000000715255737
PERFORMANCE ON TEST SET: Batch Loss = 2.4179773330688477, Accuracy = 0.1852731555700302
Training iter #180000:   Batch Loss = 2.402665, Accuracy = 0.18333333730697632
PERFORMANCE ON TEST SET: Batch Loss = 2.40043306350708, Accuracy = 0.17746861279010773
Training iter #210000:   Batch Loss = 2.379981, Accuracy = 0.17866666615009308
PERFORMANCE ON TEST SET: Batch Loss = 2.3845887184143066, Accuracy = 0.17543263733386993
Training iter #240000:   Batch Loss = 2.354426, Accuracy = 0.2006666660308838
PERFORMANCE ON TEST SET: Batch Loss = 2.3687329292297363, Accuracy = 0.18221920728683472
Training iter #270000:   Batch Loss = 2.356700, Accuracy = 0.18266665935516357
PERFORMANCE ON TEST SET: Batch Loss = 2.3544068336486816, Accuracy = 0.18425516784191132
Training iter #300000:   Batch Loss = 2.337363, Accuracy = 0.1860000044107437
PERFORMANCE ON TEST SET: Batch Loss = 2.3412837982177734, Accuracy = 0.1866304725408554
Training iter #330000:   Batch Loss = 2.327539, Accuracy = 0.17266666889190674
PERFORMANCE ON TEST SET: Batch Loss = 2.32985258102417, Accuracy = 0.17950458824634552
Training iter #360000:   Batch Loss = 2.302912, Accuracy = 0.19733333587646484
PERFORMANCE ON TEST SET: Batch Loss = 2.3171122074127197, Accuracy = 0.18018323183059692
Training iter #390000:   Batch Loss = 2.309716, Accuracy = 0.18266665935516357
PERFORMANCE ON TEST SET: Batch Loss = 2.3058035373687744, Accuracy = 0.1832371950149536
Training iter #420000:   Batch Loss = 2.288020, Accuracy = 0.17000000178813934
PERFORMANCE ON TEST SET: Batch Loss = 2.2932193279266357, Accuracy = 0.1832371950149536
Training iter #450000:   Batch Loss = 2.287873, Accuracy = 0.17266666889190674
PERFORMANCE ON TEST SET: Batch Loss = 2.281813621520996, Accuracy = 0.18459449708461761
Training iter #480000:   Batch Loss = 2.273160, Accuracy = 0.18533332645893097
PERFORMANCE ON TEST SET: Batch Loss = 2.2717151641845703, Accuracy = 0.18187987804412842
Training iter #510000:   Batch Loss = 2.262898, Accuracy = 0.18000000715255737
PERFORMANCE ON TEST SET: Batch Loss = 2.2581629753112793, Accuracy = 0.17509330809116364
Training iter #540000:   Batch Loss = 2.246558, Accuracy = 0.19200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 2.247668981552124, Accuracy = 0.17611129581928253
Training iter #570000:   Batch Loss = 2.227736, Accuracy = 0.1899999976158142
PERFORMANCE ON TEST SET: Batch Loss = 2.2377424240112305, Accuracy = 0.17237868905067444
Training iter #600000:   Batch Loss = 2.212739, Accuracy = 0.18133333325386047
PERFORMANCE ON TEST SET: Batch Loss = 2.2256546020507812, Accuracy = 0.18086189031600952
Training iter #630000:   Batch Loss = 2.207124, Accuracy = 0.1979999989271164
PERFORMANCE ON TEST SET: Batch Loss = 2.2179505825042725, Accuracy = 0.18120121955871582
Training iter #660000:   Batch Loss = 2.202742, Accuracy = 0.19200000166893005
PERFORMANCE ON TEST SET: Batch Loss = 2.2081398963928223, Accuracy = 0.1896844208240509
Training iter #690000:   Batch Loss = 2.195266, Accuracy = 0.20600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 2.1990373134613037, Accuracy = 0.1883271187543869
Training iter #720000:   Batch Loss = 2.177224, Accuracy = 0.20399999618530273
PERFORMANCE ON TEST SET: Batch Loss = 2.1907882690429688, Accuracy = 0.17712928354740143
Training iter #750000:   Batch Loss = 2.177166, Accuracy = 0.19066666066646576
PERFORMANCE ON TEST SET: Batch Loss = 2.182034730911255, Accuracy = 0.18120121955871582
Training iter #780000:   Batch Loss = 2.185475, Accuracy = 0.15466666221618652
PERFORMANCE ON TEST SET: Batch Loss = 2.177746057510376, Accuracy = 0.1835765242576599
Training iter #810000:   Batch Loss = 2.174417, Accuracy = 0.16466666758060455
PERFORMANCE ON TEST SET: Batch Loss = 2.1701595783233643, Accuracy = 0.17848660051822662
Training iter #840000:   Batch Loss = 2.162606, Accuracy = 0.17866666615009308
PERFORMANCE ON TEST SET: Batch Loss = 2.161219835281372, Accuracy = 0.17000339925289154
Training iter #870000:   Batch Loss = 2.150786, Accuracy = 0.18533332645893097
PERFORMANCE ON TEST SET: Batch Loss = 2.152496337890625, Accuracy = 0.17984390258789062
Training iter #900000:   Batch Loss = 2.148773, Accuracy = 0.17933332920074463
PERFORMANCE ON TEST SET: Batch Loss = 2.145195484161377, Accuracy = 0.18289786577224731
Training iter #930000:   Batch Loss = 2.135595, Accuracy = 0.18133333325386047
PERFORMANCE ON TEST SET: Batch Loss = 2.1359848976135254, Accuracy = 0.17984390258789062
Training iter #960000:   Batch Loss = 2.119425, Accuracy = 0.19466666877269745
PERFORMANCE ON TEST SET: Batch Loss = 2.131377696990967, Accuracy = 0.17543263733386993
Training iter #990000:   Batch Loss = 2.110783, Accuracy = 0.20133332908153534
PERFORMANCE ON TEST SET: Batch Loss = 2.1261487007141113, Accuracy = 0.18221920728683472
Training iter #1020000:   Batch Loss = 2.104097, Accuracy = 0.21400000154972076
PERFORMANCE ON TEST SET: Batch Loss = 2.1190459728240967, Accuracy = 0.1835765242576599
Training iter #1050000:   Batch Loss = 2.102142, Accuracy = 0.20466665923595428
PERFORMANCE ON TEST SET: Batch Loss = 2.112886905670166, Accuracy = 0.18459449708461761
Training iter #1080000:   Batch Loss = 2.098278, Accuracy = 0.20600000023841858
PERFORMANCE ON TEST SET: Batch Loss = 2.1070897579193115, Accuracy = 0.1839158535003662
Training iter #1110000:   Batch Loss = 2.091059, Accuracy = 0.1926666647195816
PERFORMANCE ON TEST SET: Batch Loss = 2.10029935836792, Accuracy = 0.18221920728683472
Training iter #1140000:   Batch Loss = 2.092412, Accuracy = 0.18799999356269836
PERFORMANCE ON TEST SET: Batch Loss = 2.0964701175689697, Accuracy = 0.17746861279010773
Training iter #1170000:   Batch Loss = 2.103669, Accuracy = 0.1706666648387909
PERFORMANCE ON TEST SET: Batch Loss = 2.0905725955963135, Accuracy = 0.17170003056526184
Training iter #1200000:   Batch Loss = 2.098007, Accuracy = 0.1613333374261856
PERFORMANCE ON TEST SET: Batch Loss = 2.083723306655884, Accuracy = 0.17577196657657623
Training iter #1230000:   Batch Loss = 2.083173, Accuracy = 0.18333333730697632
PERFORMANCE ON TEST SET: Batch Loss = 2.079178810119629, Accuracy = 0.17203935980796814
Training iter #1260000:   Batch Loss = 2.070014, Accuracy = 0.1913333386182785
PERFORMANCE ON TEST SET: Batch Loss = 2.071603298187256, Accuracy = 0.1832371950149536
Training iter #1290000:   Batch Loss = 2.061195, Accuracy = 0.1706666648387909
PERFORMANCE ON TEST SET: Batch Loss = 2.068068504333496, Accuracy = 0.18187987804412842
Training iter #1320000:   Batch Loss = 2.058851, Accuracy = 0.17399999499320984
PERFORMANCE ON TEST SET: Batch Loss = 2.0627498626708984, Accuracy = 0.188666433095932
Training iter #1350000:   Batch Loss = 2.046293, Accuracy = 0.22333332896232605
PERFORMANCE ON TEST SET: Batch Loss = 2.0593366622924805, Accuracy = 0.1856124848127365
Training iter #1380000:   Batch Loss = 2.036530, Accuracy = 0.18733333051204681
PERFORMANCE ON TEST SET: Batch Loss = 2.055450201034546, Accuracy = 0.17373600602149963
Training iter #1410000:   Batch Loss = 2.043997, Accuracy = 0.18799999356269836
PERFORMANCE ON TEST SET: Batch Loss = 2.051445484161377, Accuracy = 0.17441466450691223
Training iter #1440000:   Batch Loss = 2.036161, Accuracy = 0.1940000057220459
PERFORMANCE ON TEST SET: Batch Loss = 2.050931930541992, Accuracy = 0.18154054880142212
Training iter #1470000:   Batch Loss = 2.029227, Accuracy = 0.19866666197776794
PERFORMANCE ON TEST SET: Batch Loss = 2.0427074432373047, Accuracy = 0.17170003056526184
Training iter #1500000:   Batch Loss = 2.035596, Accuracy = 0.18333333730697632
PERFORMANCE ON TEST SET: Batch Loss = 2.0371153354644775, Accuracy = 0.1835765242576599
Training iter #1530000:   Batch Loss = 2.032263, Accuracy = 0.18066667020320892
PERFORMANCE ON TEST SET: Batch Loss = 2.0342512130737305, Accuracy = 0.18120121955871582
Training iter #1560000:   Batch Loss = 2.035908, Accuracy = 0.17466667294502258
PERFORMANCE ON TEST SET: Batch Loss = 2.0299482345581055, Accuracy = 0.17611129581928253
Training iter #1590000:   Batch Loss = 2.031745, Accuracy = 0.17733334004878998
PERFORMANCE ON TEST SET: Batch Loss = 2.026405096054077, Accuracy = 0.1835765242576599
Training iter #1620000:   Batch Loss = 2.027457, Accuracy = 0.16466666758060455
PERFORMANCE ON TEST SET: Batch Loss = 2.021310329437256, Accuracy = 0.18459449708461761
Training iter #1650000:   Batch Loss = 2.009004, Accuracy = 0.19599999487400055
PERFORMANCE ON TEST SET: Batch Loss = 2.016293525695801, Accuracy = 0.1862911432981491
Training iter #1680000:   Batch Loss = 2.002970, Accuracy = 0.19866666197776794
PERFORMANCE ON TEST SET: Batch Loss = 2.014127731323242, Accuracy = 0.1852731555700302
Training iter #1710000:   Batch Loss = 1.992188, Accuracy = 0.20399999618530273
PERFORMANCE ON TEST SET: Batch Loss = 2.0100133419036865, Accuracy = 0.1852731555700302
Training iter #1740000:   Batch Loss = 1.988552, Accuracy = 0.21400000154972076
PERFORMANCE ON TEST SET: Batch Loss = 2.0063281059265137, Accuracy = 0.17916525900363922
Training iter #1770000:   Batch Loss = 1.994368, Accuracy = 0.19066666066646576
PERFORMANCE ON TEST SET: Batch Loss = 2.003634214401245, Accuracy = 0.17848660051822662
Training iter #1800000:   Batch Loss = 1.995858, Accuracy = 0.18266665935516357
PERFORMANCE ON TEST SET: Batch Loss = 2.001114845275879, Accuracy = 0.17780794203281403
Training iter #1830000:   Batch Loss = 1.991144, Accuracy = 0.18933333456516266
PERFORMANCE ON TEST SET: Batch Loss = 2.0000393390655518, Accuracy = 0.17848660051822662
Training iter #1860000:   Batch Loss = 1.985905, Accuracy = 0.195333331823349
PERFORMANCE ON TEST SET: Batch Loss = 1.9954590797424316, Accuracy = 0.17509330809116364
Training iter #1890000:   Batch Loss = 2.000002, Accuracy = 0.17399999499320984
PERFORMANCE ON TEST SET: Batch Loss = 1.9920105934143066, Accuracy = 0.18459449708461761
Training iter #1920000:   Batch Loss = 1.994099, Accuracy = 0.17133332788944244
PERFORMANCE ON TEST SET: Batch Loss = 1.9887398481369019, Accuracy = 0.1866304725408554
Training iter #1950000:   Batch Loss = 1.989217, Accuracy = 0.17533333599567413
PERFORMANCE ON TEST SET: Batch Loss = 1.984339714050293, Accuracy = 0.1869698017835617
Training iter #1980000:   Batch Loss = 1.982547, Accuracy = 0.17933332920074463
PERFORMANCE ON TEST SET: Batch Loss = 1.9828863143920898, Accuracy = 0.18255853652954102
Training iter #2010000:   Batch Loss = 1.983533, Accuracy = 0.17533333599567413
PERFORMANCE ON TEST SET: Batch Loss = 1.979530930519104, Accuracy = 0.18221920728683472
Training iter #2040000:   Batch Loss = 1.975593, Accuracy = 0.17666666209697723
PERFORMANCE ON TEST SET: Batch Loss = 1.9763866662979126, Accuracy = 0.18154054880142212
Training iter #2070000:   Batch Loss = 1.955580, Accuracy = 0.20466665923595428
PERFORMANCE ON TEST SET: Batch Loss = 1.974021077156067, Accuracy = 0.17882592976093292
Training iter #2100000:   Batch Loss = 1.956438, Accuracy = 0.19333332777023315
PERFORMANCE ON TEST SET: Batch Loss = 1.9713356494903564, Accuracy = 0.1852731555700302
Training iter #2130000:   Batch Loss = 1.958149, Accuracy = 0.19866666197776794
PERFORMANCE ON TEST SET: Batch Loss = 1.969076156616211, Accuracy = 0.187309131026268
Training iter #2160000:   Batch Loss = 1.958782, Accuracy = 0.1926666647195816
PERFORMANCE ON TEST SET: Batch Loss = 1.9665125608444214, Accuracy = 0.17882592976093292
Training iter #2190000:   Batch Loss = 1.958441, Accuracy = 0.20333333313465118
PERFORMANCE ON TEST SET: Batch Loss = 1.9641164541244507, Accuracy = 0.17882592976093292
Optimization Finished!
FINAL RESULT: Batch Loss = 1.9646251201629639, Accuracy = 0.18086189031600952



Testing Accuracy: 18.086189031600952%

Precision: 8.406533480572909%
Recall: 18.08618934509671%
f1_score: 8.954245764503419%

Confusion Matrix:
[[  0   0   0  69  31 396]
 [  0   0   0  62  40 369]
 [  0   0   0  60  40 320]
 [  0   0   0  75  39 377]
 [  0   0   0  79  53 400]
 [  0   0   0  94  37 406]]

Confusion matrix (normalised to % of total test data):
[[ 0.         0.         0.         2.3413641  1.0519172 13.437393 ]
 [ 0.         0.         0.         2.1038344  1.3573124 12.521208 ]
 [ 0.         0.         0.         2.0359688  1.3573124 10.8585   ]
 [ 0.         0.         0.         2.544961   1.3233796 12.792671 ]
 [ 0.         0.         0.         2.6806922  1.798439  13.573125 ]
 [ 0.         0.         0.         3.1896844  1.2555141 13.776723 ]]