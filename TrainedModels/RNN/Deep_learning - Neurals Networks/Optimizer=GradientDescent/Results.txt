Training iter #1500:   Batch Loss = 3.363385, Accuracy = 0.1966666728258133
PERFORMANCE ON TEST SET: Batch Loss = 3.224209785461426, Accuracy = 0.16830675303936005
Training iter #30000:   Batch Loss = 2.351385, Accuracy = 0.38866665959358215
PERFORMANCE ON TEST SET: Batch Loss = 2.425854206085205, Accuracy = 0.285035640001297
Training iter #60000:   Batch Loss = 2.115767, Accuracy = 0.5246666669845581
PERFORMANCE ON TEST SET: Batch Loss = 2.1589150428771973, Accuracy = 0.46148625016212463
Training iter #90000:   Batch Loss = 1.977759, Accuracy = 0.5073333382606506
PERFORMANCE ON TEST SET: Batch Loss = 2.0052061080932617, Accuracy = 0.49066847562789917
Training iter #120000:   Batch Loss = 1.831564, Accuracy = 0.5706666707992554
PERFORMANCE ON TEST SET: Batch Loss = 1.9058210849761963, Accuracy = 0.49983033537864685
Training iter #150000:   Batch Loss = 1.771462, Accuracy = 0.5799999833106995
PERFORMANCE ON TEST SET: Batch Loss = 1.8301239013671875, Accuracy = 0.5212079882621765
Training iter #180000:   Batch Loss = 1.740445, Accuracy = 0.578000009059906
PERFORMANCE ON TEST SET: Batch Loss = 1.7814061641693115, Accuracy = 0.5235832929611206
Training iter #210000:   Batch Loss = 1.651380, Accuracy = 0.581333339214325
PERFORMANCE ON TEST SET: Batch Loss = 1.729114294052124, Accuracy = 0.5344418287277222
Training iter #240000:   Batch Loss = 1.484553, Accuracy = 0.6453333497047424
PERFORMANCE ON TEST SET: Batch Loss = 1.6973470449447632, Accuracy = 0.5446216464042664
Training iter #270000:   Batch Loss = 1.503993, Accuracy = 0.628000020980835
PERFORMANCE ON TEST SET: Batch Loss = 1.6312792301177979, Accuracy = 0.5731251835823059
Training iter #300000:   Batch Loss = 1.523966, Accuracy = 0.6119999885559082
PERFORMANCE ON TEST SET: Batch Loss = 1.645264744758606, Accuracy = 0.5704106092453003
Training iter #330000:   Batch Loss = 1.569954, Accuracy = 0.5933333039283752
PERFORMANCE ON TEST SET: Batch Loss = 1.6392511129379272, Accuracy = 0.5744825005531311
Training iter #360000:   Batch Loss = 1.408938, Accuracy = 0.6953333616256714
PERFORMANCE ON TEST SET: Batch Loss = 1.5421522855758667, Accuracy = 0.6097726225852966
Training iter #390000:   Batch Loss = 1.468485, Accuracy = 0.6633333563804626
PERFORMANCE ON TEST SET: Batch Loss = 1.52576744556427, Accuracy = 0.6094332933425903
Training iter #420000:   Batch Loss = 1.370793, Accuracy = 0.6586666703224182
PERFORMANCE ON TEST SET: Batch Loss = 1.5032883882522583, Accuracy = 0.6182558536529541
Training iter #450000:   Batch Loss = 1.398771, Accuracy = 0.637333333492279
PERFORMANCE ON TEST SET: Batch Loss = 1.4812095165252686, Accuracy = 0.628096342086792
Training iter #480000:   Batch Loss = 1.381299, Accuracy = 0.6293333172798157
PERFORMANCE ON TEST SET: Batch Loss = 1.468656063079834, Accuracy = 0.6308109760284424
Training iter #510000:   Batch Loss = 1.353791, Accuracy = 0.6506666541099548
PERFORMANCE ON TEST SET: Batch Loss = 1.4555939435958862, Accuracy = 0.6423481702804565
Training iter #540000:   Batch Loss = 1.344619, Accuracy = 0.6546666622161865
PERFORMANCE ON TEST SET: Batch Loss = 1.4472407102584839, Accuracy = 0.6450628042221069
Training iter #570000:   Batch Loss = 1.318780, Accuracy = 0.6946666836738586
PERFORMANCE ON TEST SET: Batch Loss = 1.4372514486312866, Accuracy = 0.6498133540153503
Training iter #600000:   Batch Loss = 1.282153, Accuracy = 0.6833333373069763
PERFORMANCE ON TEST SET: Batch Loss = 1.462010145187378, Accuracy = 0.6555819511413574
Training iter #630000:   Batch Loss = 1.274689, Accuracy = 0.6773333549499512
PERFORMANCE ON TEST SET: Batch Loss = 1.4273780584335327, Accuracy = 0.6633865237236023
Training iter #660000:   Batch Loss = 1.246788, Accuracy = 0.7173333168029785
PERFORMANCE ON TEST SET: Batch Loss = 1.425724983215332, Accuracy = 0.6661010980606079
Training iter #690000:   Batch Loss = 1.288220, Accuracy = 0.6959999799728394
PERFORMANCE ON TEST SET: Batch Loss = 1.4552693367004395, Accuracy = 0.6545639634132385
Training iter #720000:   Batch Loss = 1.267672, Accuracy = 0.7526666522026062
PERFORMANCE ON TEST SET: Batch Loss = 1.4166374206542969, Accuracy = 0.6674584150314331
Training iter #750000:   Batch Loss = 1.288310, Accuracy = 0.7266666889190674
PERFORMANCE ON TEST SET: Batch Loss = 1.4414455890655518, Accuracy = 0.6640651226043701
Training iter #780000:   Batch Loss = 1.278038, Accuracy = 0.7106666564941406
PERFORMANCE ON TEST SET: Batch Loss = 1.4288991689682007, Accuracy = 0.6661010980606079
Training iter #810000:   Batch Loss = 1.283057, Accuracy = 0.7106666564941406
PERFORMANCE ON TEST SET: Batch Loss = 1.4103307723999023, Accuracy = 0.671869695186615
Training iter #840000:   Batch Loss = 1.324573, Accuracy = 0.6886666417121887
PERFORMANCE ON TEST SET: Batch Loss = 1.420637845993042, Accuracy = 0.6820495128631592
Training iter #870000:   Batch Loss = 1.309398, Accuracy = 0.6846666932106018
PERFORMANCE ON TEST SET: Batch Loss = 1.4080889225006104, Accuracy = 0.6813709139823914
Training iter #900000:   Batch Loss = 1.248710, Accuracy = 0.7133333086967468
PERFORMANCE ON TEST SET: Batch Loss = 1.3842229843139648, Accuracy = 0.6847641468048096
Training iter #930000:   Batch Loss = 1.294901, Accuracy = 0.6926666498184204
PERFORMANCE ON TEST SET: Batch Loss = 1.440945029258728, Accuracy = 0.6671190857887268
Training iter #960000:   Batch Loss = 1.208354, Accuracy = 0.7580000162124634
PERFORMANCE ON TEST SET: Batch Loss = 1.3905638456344604, Accuracy = 0.6952833533287048
Training iter #990000:   Batch Loss = 1.139462, Accuracy = 0.7713333368301392
PERFORMANCE ON TEST SET: Batch Loss = 1.3897595405578613, Accuracy = 0.6976586580276489
Training iter #1020000:   Batch Loss = 1.176729, Accuracy = 0.7453333139419556
PERFORMANCE ON TEST SET: Batch Loss = 1.3731415271759033, Accuracy = 0.6956226825714111
Training iter #1050000:   Batch Loss = 1.249920, Accuracy = 0.6966666579246521
PERFORMANCE ON TEST SET: Batch Loss = 1.4147597551345825, Accuracy = 0.6745843291282654
Training iter #1080000:   Batch Loss = 1.184491, Accuracy = 0.8013333082199097
PERFORMANCE ON TEST SET: Batch Loss = 1.3681226968765259, Accuracy = 0.7003732323646545
Training iter #1110000:   Batch Loss = 1.206854, Accuracy = 0.7733333110809326
PERFORMANCE ON TEST SET: Batch Loss = 1.3937797546386719, Accuracy = 0.6857821345329285
Training iter #1140000:   Batch Loss = 1.238433, Accuracy = 0.7620000243186951
PERFORMANCE ON TEST SET: Batch Loss = 1.414705514907837, Accuracy = 0.6745843291282654
Training iter #1170000:   Batch Loss = 1.726424, Accuracy = 0.5479999780654907
PERFORMANCE ON TEST SET: Batch Loss = 1.742118000984192, Accuracy = 0.5894129872322083
Training iter #1200000:   Batch Loss = 1.382552, Accuracy = 0.6786666512489319
PERFORMANCE ON TEST SET: Batch Loss = 1.5958852767944336, Accuracy = 0.5965388417243958
Training iter #1230000:   Batch Loss = 1.387033, Accuracy = 0.7046666741371155
PERFORMANCE ON TEST SET: Batch Loss = 1.5322660207748413, Accuracy = 0.6538853049278259
Training iter #1260000:   Batch Loss = 1.270494, Accuracy = 0.7213333249092102
PERFORMANCE ON TEST SET: Batch Loss = 1.443284511566162, Accuracy = 0.6813709139823914
Training iter #1290000:   Batch Loss = 1.346619, Accuracy = 0.6679999828338623
PERFORMANCE ON TEST SET: Batch Loss = 1.425356149673462, Accuracy = 0.6918900609016418
Training iter #1320000:   Batch Loss = 1.261067, Accuracy = 0.7486666440963745
PERFORMANCE ON TEST SET: Batch Loss = 1.4023082256317139, Accuracy = 0.6749236583709717
Training iter #1350000:   Batch Loss = 1.679521, Accuracy = 0.5666666626930237
PERFORMANCE ON TEST SET: Batch Loss = 1.7561297416687012, Accuracy = 0.5127248167991638
Training iter #1380000:   Batch Loss = 1.246232, Accuracy = 0.7413333058357239
PERFORMANCE ON TEST SET: Batch Loss = 1.4525855779647827, Accuracy = 0.6667797565460205
Training iter #1410000:   Batch Loss = 1.252134, Accuracy = 0.7333333492279053
PERFORMANCE ON TEST SET: Batch Loss = 1.471250057220459, Accuracy = 0.65626060962677
Training iter #1440000:   Batch Loss = 1.240933, Accuracy = 0.7393333315849304
PERFORMANCE ON TEST SET: Batch Loss = 1.4281541109085083, Accuracy = 0.6762809753417969
Training iter #1470000:   Batch Loss = 1.216917, Accuracy = 0.7866666913032532
PERFORMANCE ON TEST SET: Batch Loss = 1.3938863277435303, Accuracy = 0.6918900609016418
Training iter #1500000:   Batch Loss = 1.253684, Accuracy = 0.7566666603088379
PERFORMANCE ON TEST SET: Batch Loss = 1.4381451606750488, Accuracy = 0.6708517074584961
Training iter #1530000:   Batch Loss = 1.187385, Accuracy = 0.75
PERFORMANCE ON TEST SET: Batch Loss = 1.4293558597564697, Accuracy = 0.6705123782157898
Training iter #1560000:   Batch Loss = 1.202266, Accuracy = 0.7513333559036255
PERFORMANCE ON TEST SET: Batch Loss = 1.403244137763977, Accuracy = 0.68747878074646
Training iter #1590000:   Batch Loss = 1.172135, Accuracy = 0.7806666493415833
PERFORMANCE ON TEST SET: Batch Loss = 1.4079065322875977, Accuracy = 0.6891754269599915
Training iter #1620000:   Batch Loss = 1.151548, Accuracy = 0.8213333487510681
PERFORMANCE ON TEST SET: Batch Loss = 1.3954508304595947, Accuracy = 0.6990159749984741
Training iter #1650000:   Batch Loss = 1.249056, Accuracy = 0.7166666388511658
PERFORMANCE ON TEST SET: Batch Loss = 1.4497052431106567, Accuracy = 0.6739056706428528
Training iter #1680000:   Batch Loss = 1.183146, Accuracy = 0.7666666507720947
PERFORMANCE ON TEST SET: Batch Loss = 1.4245432615280151, Accuracy = 0.6973193287849426
Training iter #1710000:   Batch Loss = 1.160597, Accuracy = 0.7773333191871643
PERFORMANCE ON TEST SET: Batch Loss = 1.3892529010772705, Accuracy = 0.7129284143447876
Training iter #1740000:   Batch Loss = 1.094574, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 1.3516896963119507, Accuracy = 0.7142857313156128
Training iter #1770000:   Batch Loss = 1.167333, Accuracy = 0.7606666684150696
PERFORMANCE ON TEST SET: Batch Loss = 1.3462116718292236, Accuracy = 0.7207329273223877
Training iter #1800000:   Batch Loss = 1.151223, Accuracy = 0.7586666941642761
PERFORMANCE ON TEST SET: Batch Loss = 1.432647943496704, Accuracy = 0.684085488319397
Training iter #1830000:   Batch Loss = 1.142040, Accuracy = 0.79666668176651
PERFORMANCE ON TEST SET: Batch Loss = 1.3959978818893433, Accuracy = 0.7003732323646545
Training iter #1860000:   Batch Loss = 1.137500, Accuracy = 0.800000011920929
PERFORMANCE ON TEST SET: Batch Loss = 1.3955152034759521, Accuracy = 0.7024092078208923
Training iter #1890000:   Batch Loss = 1.135355, Accuracy = 0.7973333597183228
PERFORMANCE ON TEST SET: Batch Loss = 1.34342360496521, Accuracy = 0.7265015244483948
Training iter #1920000:   Batch Loss = 1.148798, Accuracy = 0.762666642665863
PERFORMANCE ON TEST SET: Batch Loss = 1.3683702945709229, Accuracy = 0.7220902442932129
Training iter #1950000:   Batch Loss = 1.099594, Accuracy = 0.8106666803359985
PERFORMANCE ON TEST SET: Batch Loss = 1.3462331295013428, Accuracy = 0.7170003652572632
Training iter #1980000:   Batch Loss = 1.119166, Accuracy = 0.8173333406448364
PERFORMANCE ON TEST SET: Batch Loss = 1.3969335556030273, Accuracy = 0.7129284143447876
Training iter #2010000:   Batch Loss = 1.108011, Accuracy = 0.8259999752044678
PERFORMANCE ON TEST SET: Batch Loss = 1.3628196716308594, Accuracy = 0.7302341461181641
Training iter #2040000:   Batch Loss = 1.131538, Accuracy = 0.7873333096504211
PERFORMANCE ON TEST SET: Batch Loss = 1.3269760608673096, Accuracy = 0.7281981706619263
Training iter #2070000:   Batch Loss = 1.073853, Accuracy = 0.8193333148956299
PERFORMANCE ON TEST SET: Batch Loss = 1.3312638998031616, Accuracy = 0.7376993298530579
Training iter #2100000:   Batch Loss = 1.115129, Accuracy = 0.7746666669845581
PERFORMANCE ON TEST SET: Batch Loss = 1.4099599123001099, Accuracy = 0.7095351219177246
Training iter #2130000:   Batch Loss = 1.055796, Accuracy = 0.7873333096504211
PERFORMANCE ON TEST SET: Batch Loss = 1.3462560176849365, Accuracy = 0.7241262197494507
Training iter #2160000:   Batch Loss = 1.208885, Accuracy = 0.7293333411216736
PERFORMANCE ON TEST SET: Batch Loss = 1.338294506072998, Accuracy = 0.733627438545227
Training iter #2190000:   Batch Loss = 1.148469, Accuracy = 0.7666666507720947
PERFORMANCE ON TEST SET: Batch Loss = 1.3234566450119019, Accuracy = 0.7329487800598145
Optimization Finished!
FINAL RESULT: Batch Loss = 1.3336560726165771, Accuracy = 0.7353240847587585


Testing Accuracy: 73.53240847587585%

Precision: 74.27520430177145%
Recall: 73.53240583644384%
f1_score: 73.59233357827138%

Confusion Matrix:
[[304  79 110   1   2   0]
 [ 78 328  64   0   1   0]
 [126  72 218   0   4   0]
 [  0  24   0 335 132   0]
 [  0   7   0  53 472   0]
 [  0  27   0   0   0 510]]

Confusion matrix (normalised to % of total test data):
[[10.315576    2.6806922   3.7326093   0.03393281  0.06786563  0.        ]
 [ 2.6467593  11.129963    2.1717      0.          0.03393281  0.        ]
 [ 4.275534    2.4431624   7.3973527   0.          0.13573125  0.        ]
 [ 0.          0.8143875   0.         11.367493    4.4791317   0.        ]
 [ 0.          0.2375297   0.          1.798439   16.016289    0.        ]
 [ 0.          0.916186    0.          0.          0.         17.305735  ]]
Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.