Training iter #1000:   Batch Loss = 2.602975, Accuracy = 0.14300000667572021
PERFORMANCE ON TEST SET: Batch Loss = 2.323436975479126, Accuracy = 0.16762809455394745
Training iter #30000:   Batch Loss = 1.360324, Accuracy = 0.628000020980835
PERFORMANCE ON TEST SET: Batch Loss = 1.4369375705718994, Accuracy = 0.5578554272651672
Training iter #60000:   Batch Loss = 0.962774, Accuracy = 0.7129999995231628
PERFORMANCE ON TEST SET: Batch Loss = 1.1413466930389404, Accuracy = 0.6423481702804565
Training iter #90000:   Batch Loss = 0.887671, Accuracy = 0.7260000109672546
PERFORMANCE ON TEST SET: Batch Loss = 1.0384323596954346, Accuracy = 0.7200542688369751
Training iter #120000:   Batch Loss = 0.624430, Accuracy = 0.8740000128746033
PERFORMANCE ON TEST SET: Batch Loss = 0.9444156885147095, Accuracy = 0.7611129879951477
Training iter #150000:   Batch Loss = 0.648746, Accuracy = 0.8569999933242798
PERFORMANCE ON TEST SET: Batch Loss = 0.8696388006210327, Accuracy = 0.7624703049659729
Training iter #180000:   Batch Loss = 0.687392, Accuracy = 0.8410000205039978
PERFORMANCE ON TEST SET: Batch Loss = 0.7940897941589355, Accuracy = 0.8113335371017456
Training iter #210000:   Batch Loss = 0.609219, Accuracy = 0.8949999809265137
PERFORMANCE ON TEST SET: Batch Loss = 0.7272633910179138, Accuracy = 0.8401764631271362
Training iter #240000:   Batch Loss = 0.428528, Accuracy = 0.9330000281333923
PERFORMANCE ON TEST SET: Batch Loss = 0.7408496141433716, Accuracy = 0.8394978046417236
Training iter #270000:   Batch Loss = 0.457096, Accuracy = 0.9359999895095825
PERFORMANCE ON TEST SET: Batch Loss = 0.8514463901519775, Accuracy = 0.8076009750366211
Training iter #300000:   Batch Loss = 0.356586, Accuracy = 0.9890000224113464
PERFORMANCE ON TEST SET: Batch Loss = 0.7201921343803406, Accuracy = 0.8523922562599182
Training iter #330000:   Batch Loss = 0.409925, Accuracy = 0.9580000042915344
PERFORMANCE ON TEST SET: Batch Loss = 0.7005274295806885, Accuracy = 0.8595181703567505
Training iter #360000:   Batch Loss = 0.484765, Accuracy = 0.9210000038146973
PERFORMANCE ON TEST SET: Batch Loss = 0.7215781807899475, Accuracy = 0.857821524143219
Training iter #390000:   Batch Loss = 0.330516, Accuracy = 0.9729999899864197
PERFORMANCE ON TEST SET: Batch Loss = 0.6848267316818237, Accuracy = 0.8703766465187073
Training iter #420000:   Batch Loss = 0.369710, Accuracy = 0.9440000057220459
PERFORMANCE ON TEST SET: Batch Loss = 0.6805321574211121, Accuracy = 0.8707159757614136
Training iter #450000:   Batch Loss = 0.415320, Accuracy = 0.921999990940094
PERFORMANCE ON TEST SET: Batch Loss = 0.6820130944252014, Accuracy = 0.8707159757614136
Training iter #480000:   Batch Loss = 0.369015, Accuracy = 0.9340000152587891
PERFORMANCE ON TEST SET: Batch Loss = 0.7074133157730103, Accuracy = 0.8625721335411072
Training iter #510000:   Batch Loss = 0.316104, Accuracy = 0.972000002861023
PERFORMANCE ON TEST SET: Batch Loss = 0.6310790181159973, Accuracy = 0.8825924396514893
Training iter #540000:   Batch Loss = 0.404989, Accuracy = 0.9039999842643738
PERFORMANCE ON TEST SET: Batch Loss = 0.6674867868423462, Accuracy = 0.8778418898582458
Training iter #570000:   Batch Loss = 0.444989, Accuracy = 0.8960000276565552
PERFORMANCE ON TEST SET: Batch Loss = 0.6595407724380493, Accuracy = 0.8805565237998962
Training iter #600000:   Batch Loss = 0.314843, Accuracy = 0.9509999752044678
PERFORMANCE ON TEST SET: Batch Loss = 0.7891944646835327, Accuracy = 0.8316932320594788
Training iter #630000:   Batch Loss = 0.316276, Accuracy = 0.9649999737739563
PERFORMANCE ON TEST SET: Batch Loss = 0.6130586266517639, Accuracy = 0.88802170753479
Training iter #660000:   Batch Loss = 0.320584, Accuracy = 0.9490000009536743
PERFORMANCE ON TEST SET: Batch Loss = 0.5795571804046631, Accuracy = 0.8931116461753845
Training iter #690000:   Batch Loss = 0.259912, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 0.5837973356246948, Accuracy = 0.894808292388916
Training iter #720000:   Batch Loss = 0.387033, Accuracy = 0.9309999942779541
PERFORMANCE ON TEST SET: Batch Loss = 0.6041516065597534, Accuracy = 0.8988802433013916
Training iter #750000:   Batch Loss = 0.342951, Accuracy = 0.9359999895095825
PERFORMANCE ON TEST SET: Batch Loss = 0.5990732908248901, Accuracy = 0.891414999961853
Training iter #780000:   Batch Loss = 0.277240, Accuracy = 0.9760000109672546
PERFORMANCE ON TEST SET: Batch Loss = 0.6182186007499695, Accuracy = 0.8832710981369019
Training iter #810000:   Batch Loss = 0.328010, Accuracy = 0.9520000219345093
PERFORMANCE ON TEST SET: Batch Loss = 0.6627712249755859, Accuracy = 0.8730912804603577
Training iter #840000:   Batch Loss = 0.345776, Accuracy = 0.9419999718666077
PERFORMANCE ON TEST SET: Batch Loss = 0.6941850185394287, Accuracy = 0.8646080493927002
Training iter #870000:   Batch Loss = 0.280613, Accuracy = 0.9679999947547913
PERFORMANCE ON TEST SET: Batch Loss = 0.575776994228363, Accuracy = 0.8917543292045593
Training iter #900000:   Batch Loss = 0.268446, Accuracy = 0.9879999756813049
PERFORMANCE ON TEST SET: Batch Loss = 0.5873057842254639, Accuracy = 0.8954869508743286
Training iter #930000:   Batch Loss = 0.335264, Accuracy = 0.9110000133514404
PERFORMANCE ON TEST SET: Batch Loss = 0.5673685073852539, Accuracy = 0.898201584815979
Training iter #960000:   Batch Loss = 0.268940, Accuracy = 0.9850000143051147
PERFORMANCE ON TEST SET: Batch Loss = 0.5445087552070618, Accuracy = 0.9039701223373413
Training iter #990000:   Batch Loss = 0.257038, Accuracy = 0.9819999933242798
PERFORMANCE ON TEST SET: Batch Loss = 0.5586001873016357, Accuracy = 0.8975229263305664
Training iter #1020000:   Batch Loss = 0.279791, Accuracy = 0.9810000061988831
PERFORMANCE ON TEST SET: Batch Loss = 0.5581345558166504, Accuracy = 0.8988802433013916
Training iter #1050000:   Batch Loss = 0.242256, Accuracy = 0.9819999933242798
PERFORMANCE ON TEST SET: Batch Loss = 0.5862645506858826, Accuracy = 0.8927723169326782
Training iter #1080000:   Batch Loss = 0.315376, Accuracy = 0.9620000123977661
PERFORMANCE ON TEST SET: Batch Loss = 0.5521543025970459, Accuracy = 0.8985409140586853
Training iter #1110000:   Batch Loss = 0.383400, Accuracy = 0.9089999794960022
PERFORMANCE ON TEST SET: Batch Loss = 0.5842799544334412, Accuracy = 0.8920936584472656
Training iter #1140000:   Batch Loss = 0.273565, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.5941011309623718, Accuracy = 0.8781812191009521
Training iter #1170000:   Batch Loss = 0.256361, Accuracy = 0.9549999833106995
PERFORMANCE ON TEST SET: Batch Loss = 0.5672366619110107, Accuracy = 0.8853070735931396
Training iter #1200000:   Batch Loss = 0.288160, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.5738027095794678, Accuracy = 0.8954869508743286
Training iter #1230000:   Batch Loss = 0.563726, Accuracy = 0.8489999771118164
PERFORMANCE ON TEST SET: Batch Loss = 0.6328569054603577, Accuracy = 0.8646080493927002
Training iter #1260000:   Batch Loss = 0.283444, Accuracy = 0.9729999899864197
PERFORMANCE ON TEST SET: Batch Loss = 0.475680410861969, Accuracy = 0.8998982310295105
Training iter #1290000:   Batch Loss = 0.340993, Accuracy = 0.9190000295639038
PERFORMANCE ON TEST SET: Batch Loss = 0.4616757035255432, Accuracy = 0.9019341468811035
Training iter #1320000:   Batch Loss = 0.320111, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.46775326132774353, Accuracy = 0.9114353656768799
Training iter #1350000:   Batch Loss = 0.222047, Accuracy = 0.9990000128746033
PERFORMANCE ON TEST SET: Batch Loss = 0.47077128291130066, Accuracy = 0.9158466458320618
Training iter #1380000:   Batch Loss = 0.260063, Accuracy = 0.9509999752044678
PERFORMANCE ON TEST SET: Batch Loss = 0.47165820002555847, Accuracy = 0.9114353656768799
Training iter #1410000:   Batch Loss = 0.240159, Accuracy = 0.9559999704360962
PERFORMANCE ON TEST SET: Batch Loss = 0.47054797410964966, Accuracy = 0.910417377948761
Training iter #1440000:   Batch Loss = 0.257878, Accuracy = 0.9769999980926514
PERFORMANCE ON TEST SET: Batch Loss = 0.48582082986831665, Accuracy = 0.9073634147644043
Training iter #1470000:   Batch Loss = 0.321358, Accuracy = 0.9309999942779541
PERFORMANCE ON TEST SET: Batch Loss = 0.48179930448532104, Accuracy = 0.9121140241622925
Training iter #1500000:   Batch Loss = 0.262045, Accuracy = 0.9290000200271606
PERFORMANCE ON TEST SET: Batch Loss = 0.49401190876960754, Accuracy = 0.9009161591529846
Training iter #1530000:   Batch Loss = 0.211171, Accuracy = 0.9710000157356262
PERFORMANCE ON TEST SET: Batch Loss = 0.4761236310005188, Accuracy = 0.9049881100654602
Training iter #1560000:   Batch Loss = 0.293346, Accuracy = 0.9240000247955322
PERFORMANCE ON TEST SET: Batch Loss = 0.5630749464035034, Accuracy = 0.891414999961853
Training iter #1590000:   Batch Loss = 0.297325, Accuracy = 0.9300000071525574
PERFORMANCE ON TEST SET: Batch Loss = 0.5009356141090393, Accuracy = 0.8890396952629089
Training iter #1620000:   Batch Loss = 0.251196, Accuracy = 0.9559999704360962
PERFORMANCE ON TEST SET: Batch Loss = 0.4742012321949005, Accuracy = 0.9029521346092224
Training iter #1650000:   Batch Loss = 0.300585, Accuracy = 0.921999990940094
PERFORMANCE ON TEST SET: Batch Loss = 0.4980226159095764, Accuracy = 0.8985409140586853
Training iter #1680000:   Batch Loss = 0.288642, Accuracy = 0.9010000228881836
PERFORMANCE ON TEST SET: Batch Loss = 0.49613893032073975, Accuracy = 0.8985409140586853
Training iter #1710000:   Batch Loss = 0.211594, Accuracy = 0.9860000014305115
PERFORMANCE ON TEST SET: Batch Loss = 0.5033956170082092, Accuracy = 0.8978622555732727
Training iter #1740000:   Batch Loss = 0.252387, Accuracy = 0.9679999947547913
PERFORMANCE ON TEST SET: Batch Loss = 0.5150797367095947, Accuracy = 0.8978622555732727
Training iter #1770000:   Batch Loss = 0.273453, Accuracy = 0.9380000233650208
PERFORMANCE ON TEST SET: Batch Loss = 0.5298097133636475, Accuracy = 0.8903970122337341
Training iter #1800000:   Batch Loss = 0.235136, Accuracy = 0.9869999885559082
PERFORMANCE ON TEST SET: Batch Loss = 0.5638158321380615, Accuracy = 0.8703766465187073
Training iter #1830000:   Batch Loss = 0.310945, Accuracy = 0.9309999942779541
PERFORMANCE ON TEST SET: Batch Loss = 0.5184445381164551, Accuracy = 0.8900576829910278
Training iter #1860000:   Batch Loss = 0.340450, Accuracy = 0.9100000262260437
PERFORMANCE ON TEST SET: Batch Loss = 0.5019639134407043, Accuracy = 0.8941296339035034
Training iter #1890000:   Batch Loss = 0.182786, Accuracy = 0.9700000286102295
PERFORMANCE ON TEST SET: Batch Loss = 0.5018868446350098, Accuracy = 0.8924329876899719
Training iter #1920000:   Batch Loss = 0.225678, Accuracy = 0.9589999914169312
PERFORMANCE ON TEST SET: Batch Loss = 0.49544113874435425, Accuracy = 0.8961656093597412
Training iter #1950000:   Batch Loss = 0.235275, Accuracy = 0.9399999976158142
PERFORMANCE ON TEST SET: Batch Loss = 0.5125356912612915, Accuracy = 0.8920936584472656
Training iter #1980000:   Batch Loss = 0.234378, Accuracy = 0.9409999847412109
PERFORMANCE ON TEST SET: Batch Loss = 0.5156283974647522, Accuracy = 0.8975229263305664
Training iter #2010000:   Batch Loss = 0.179212, Accuracy = 1.0
PERFORMANCE ON TEST SET: Batch Loss = 0.5124087333679199, Accuracy = 0.8944689631462097
Training iter #2040000:   Batch Loss = 0.261106, Accuracy = 0.9279999732971191
PERFORMANCE ON TEST SET: Batch Loss = 0.5250536203384399, Accuracy = 0.894808292388916
Training iter #2070000:   Batch Loss = 0.249332, Accuracy = 0.9869999885559082
PERFORMANCE ON TEST SET: Batch Loss = 0.5653078556060791, Accuracy = 0.8754665851593018
Training iter #2100000:   Batch Loss = 0.182596, Accuracy = 0.9990000128746033
PERFORMANCE ON TEST SET: Batch Loss = 0.4657691717147827, Accuracy = 0.9005768299102783
Training iter #2130000:   Batch Loss = 0.213666, Accuracy = 0.9480000138282776
PERFORMANCE ON TEST SET: Batch Loss = 0.47472167015075684, Accuracy = 0.8992195725440979
Training iter #2160000:   Batch Loss = 0.214324, Accuracy = 0.9419999718666077
PERFORMANCE ON TEST SET: Batch Loss = 0.4873475432395935, Accuracy = 0.8920936584472656
Training iter #2190000:   Batch Loss = 0.237574, Accuracy = 0.9739999771118164
PERFORMANCE ON TEST SET: Batch Loss = 0.5406419634819031, Accuracy = 0.884628415107727
Optimization Finished!
FINAL RESULT: Batch Loss = 0.5059237480163574, Accuracy = 0.8829317688941956



Testing Accuracy: 88.29317688941956%

Precision: 89.03198186523477%
Recall: 88.29317950458093%
f1_score: 88.26917846723397%


Confusion Matrix:
[[457   9  27   2   1   0]
 [ 29 440   2   0   0   0]
 [  3   8 409   0   0   0]
 [  2   3   0 421  65   0]
 [  3   5   0 110 414   0]
 [  0   0   0   0   0 537]]

Confusion matrix (normalised to % of total test data):
[[15.507296    0.3053953   0.916186    0.06786563  0.03393281  0.        ]
 [ 0.9840515  14.930437    0.06786563  0.          0.          0.        ]
 [ 0.10179844  0.2714625  13.87852     0.          0.          0.        ]
 [ 0.06786563  0.10179844  0.         14.285715    2.205633    0.        ]
 [ 0.10179844  0.16966406  0.          3.7326093  14.048184    0.        ]
 [ 0.          0.          0.          0.          0.         18.22192   ]]
Note: training and testing data is not equally distributed amongst classes, 
so it is normal that more than a 6th of the data is correctly classifier in the last category.